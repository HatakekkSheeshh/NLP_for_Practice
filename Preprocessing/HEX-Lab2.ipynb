{"cells":[{"cell_type":"markdown","metadata":{"id":"kFD7DVZ-xKdT"},"source":["# Homework Lab 2: Text Preprocessing with Vietnamese\n","**Overview:** In this exercise, we will build a text preprocessing program for Vietnamese."]},{"cell_type":"markdown","metadata":{"id":"XOAeiqdrxKdt"},"source":["Import the necessary libraries. Note that we are using the underthesea library for Vietnamese tokenization. To install it, follow the instructions below. ([link](https://github.com/undertheseanlp/underthesea))"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vCmzazMMZtIl","outputId":"686565a2-cc23-4298-d8cb-3c61ab92c89c","executionInfo":{"status":"ok","timestamp":1769420099135,"user_tz":-420,"elapsed":3720,"user":{"displayName":"Quốc Hiệu Nguyễn","userId":"03425075822245856665"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: underthesea in /usr/local/lib/python3.12/dist-packages (9.1.4)\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.12/dist-packages (from underthesea) (8.3.1)\n","Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.12/dist-packages (from underthesea) (0.9.12)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from underthesea) (4.67.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from underthesea) (2.32.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from underthesea) (1.5.3)\n","Requirement already satisfied: scikit-learn>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from underthesea) (1.6.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from underthesea) (6.0.3)\n","Requirement already satisfied: underthesea_core>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from underthesea) (1.0.7)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from underthesea) (0.36.0)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.1->underthesea) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.1->underthesea) (1.16.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.1->underthesea) (3.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (3.20.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (25.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (1.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (2026.1.4)\n"]}],"source":["!pip install underthesea"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"RrFQ_Ht_xKdu","executionInfo":{"status":"ok","timestamp":1769420099162,"user_tz":-420,"elapsed":14,"user":{"displayName":"Quốc Hiệu Nguyễn","userId":"03425075822245856665"}}},"outputs":[],"source":["import os,glob\n","import codecs\n","import sys\n","import re\n","from underthesea import word_tokenize"]},{"cell_type":"markdown","metadata":{"id":"hC27lBQZxKdw"},"source":["## Question 1: Create a Corpus and Survey the Data\n","\n","The data in this section is partially extracted from the [VNTC](https://github.com/duyvuleo/VNTC) dataset. VNTC is a Vietnamese news dataset covering various topics. In this section, we will only process the science topic from VNTC. We will create a corpus from both the train and test directories. Complete the following program:\n","\n","- Write `sentences_list` to a file named `dataset_name.txt`, with each element as a document on a separate line.\n","- Check how many documents are in the corpus.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"NQu_bjVgtZNE","outputId":"d59dae3a-7684-4040-d94b-af7969c735a7","executionInfo":{"status":"ok","timestamp":1769420127234,"user_tz":-420,"elapsed":28063,"user":{"displayName":"Quốc Hiệu Nguyễn","userId":"03425075822245856665"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2026-01-26 09:34:59--  https://github.com/duyvuleo/VNTC/raw/master/Data/10Topics/Ver1.1/Train_Full.rar\n","Resolving github.com (github.com)... 140.82.113.3\n","Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/duyvuleo/VNTC/master/Data/10Topics/Ver1.1/Train_Full.rar [following]\n","--2026-01-26 09:34:59--  https://raw.githubusercontent.com/duyvuleo/VNTC/master/Data/10Topics/Ver1.1/Train_Full.rar\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 49152721 (47M) [application/octet-stream]\n","Saving to: ‘Train_Full.rar.2’\n","\n","Train_Full.rar.2    100%[===================>]  46.88M   199MB/s    in 0.2s    \n","\n","2026-01-26 09:35:00 (199 MB/s) - ‘Train_Full.rar.2’ saved [49152721/49152721]\n","\n","--2026-01-26 09:35:00--  https://github.com/duyvuleo/VNTC/raw/master/Data/10Topics/Ver1.1/Test_Full.rar\n","Resolving github.com (github.com)... 140.82.112.3\n","Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/duyvuleo/VNTC/master/Data/10Topics/Ver1.1/Test_Full.rar [following]\n","--2026-01-26 09:35:00--  https://raw.githubusercontent.com/duyvuleo/VNTC/master/Data/10Topics/Ver1.1/Test_Full.rar\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 77254847 (74M) [application/octet-stream]\n","Saving to: ‘Test_Full.rar.2’\n","\n","Test_Full.rar.2     100%[===================>]  73.68M   148MB/s    in 0.5s    \n","\n","2026-01-26 09:35:02 (148 MB/s) - ‘Test_Full.rar.2’ saved [77254847/77254847]\n","\n","\n","UNRAR 6.11 beta 1 freeware      Copyright (c) 1993-2022 Alexander Roshal\n","\n","\n","Extracting from Train_Full.rar\n","\n","\n","Would you like to replace the existing file VNTC_khoahoc/Train_Full/Chinh tri Xa hoi/XH_VNE_ (30).txt\n","  5958 bytes, modified on 2006-06-01 15:48\n","with a new one\n","  5958 bytes, modified on 2006-06-01 15:48\n","\n","[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit q\n","\n","Program aborted\n","\n","UNRAR 6.11 beta 1 freeware      Copyright (c) 1993-2022 Alexander Roshal\n","\n","\n","Extracting from Test_Full.rar\n","\n","\n","Would you like to replace the existing file VNTC_khoahoc/Test_Full/Kinh doanh/KD_VNE_T_ (1792).txt\n","  3926 bytes, modified on 2006-06-01 21:03\n","with a new one\n","  3926 bytes, modified on 2006-06-01 21:03\n","\n","[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit q\n","\n","Program aborted\n"]}],"source":["!mkdir -p VNTC_khoahoc\n","\n","train_url = \"https://github.com/duyvuleo/VNTC/raw/master/Data/10Topics/Ver1.1/Train_Full.rar\"\n","test_url = \"https://github.com/duyvuleo/VNTC/raw/master/Data/10Topics/Ver1.1/Test_Full.rar\"\n","!wget {train_url}\n","!wget {test_url}\n","\n","!unrar x Train_Full.rar VNTC_khoahoc/\n","!unrar x Test_Full.rar VNTC_khoahoc/"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GyNKT8wAxKdx","outputId":"c67aecfa-602a-49db-c115-582f4ff020e6","executionInfo":{"status":"ok","timestamp":1769420127459,"user_tz":-420,"elapsed":218,"user":{"displayName":"Quốc Hiệu Nguyễn","userId":"03425075822245856665"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["train labels = test labels\n","Number documents in the corpus: 3916\n"]}],"source":["dataset_name = \"VNTC_khoahoc\"\n","\n","path = ['./VNTC_khoahoc/Train_Full/', './VNTC_khoahoc/Test_Full/']\n","\n","if os.listdir(path[0]) == os.listdir(path[1]):\n","    folder_list = [os.listdir(path[0]), os.listdir(path[1])]\n","    print(\"train labels = test labels\")\n","else:\n","    print(\"train labels differ from test labels\")\n","\n","doc_num = 0\n","sentences_list = []\n","meta_data_list = []\n","for i in range(2):\n","    # for folder_name in folder_list[i]:\n","    folder_path = path[i] + \"Khoa hoc\"\n","    # if folder_name[0] != \".\":\n","    if os.path.exists(folder_path):\n","      for file_name in glob.glob(os.path.join(folder_path, '*.txt')):\n","          # Read the file content into f\n","          f = codecs.open(file_name, 'br')\n","          # Convert the data to UTF-16 format for Vietnamese text\n","          file_content = (f.read().decode(\"utf-16\")).replace(\"\\r\\n\", \" \")\n","          sentences_list.append(file_content.strip())\n","          f.close\n","          # Count the number of documents\n","          doc_num += 1\n","\n","#### YOUR CODE HERE ####\n","with open(dataset_name + \".txt\", \"w\", encoding=\"utf-8\") as f:\n","  for doc in sentences_list:\n","    f.write(doc + \"\\n\")\n","\n","# Check number of the corpus\n","print(f\"Number documents in the corpus: {doc_num}\")\n","\n","#### END YOUR CODE #####"]},{"cell_type":"markdown","metadata":{"id":"TBBGzrVlZtIp"},"source":["## Question 2: Write Preprocessing Functions\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3KXHcDpuxKd0"},"source":["### Question 2.1: Write a Function to Clean Text\n","Hint:\n","- The text should only retain the following characters: aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9(),!?\\'\\\n","- Then trim the whitespace in the input text."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"k8hIglDXxKd0","executionInfo":{"status":"ok","timestamp":1769420127504,"user_tz":-420,"elapsed":16,"user":{"displayName":"Quốc Hiệu Nguyễn","userId":"03425075822245856665"}}},"outputs":[],"source":["def clean_str(string):\n","    #### YOUR CODE HERE ####\n","    regex = r\"[^aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9(),!?'\\\"]\"\n","    return re.sub(regex, ' ', string).strip()\n","    #### END YOUR CODE #####\n","# print(clean_str(\"abĂbÂ!!!!*&\"))"]},{"cell_type":"markdown","metadata":{"id":"9KfXstqAxKd1"},"source":["### Question 2.2: Write a Function to Convert Text to Lowercase"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"KRwgVjxhxKd1","executionInfo":{"status":"ok","timestamp":1769420127564,"user_tz":-420,"elapsed":57,"user":{"displayName":"Quốc Hiệu Nguyễn","userId":"03425075822245856665"}}},"outputs":[],"source":["# make all text lowercase\n","def text_lowercase(string):\n","    #### YOUR CODE HERE ###\n","    # str -> str\n","    return string.lower()\n","    #### END YOUR CODE #####\n","# print(text_lowercase(clean_str(\"aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9(),!?'\\\"]\")))"]},{"cell_type":"markdown","metadata":{"id":"rYM_GO_5xKd2"},"source":["### Question 2.3: Tokenize Words\n","Hint: Use the `word_tokenize()` function imported above with two parameters: `strings` and `format=\"text\"`.\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"pty34NwyxKd2","executionInfo":{"status":"ok","timestamp":1769420127567,"user_tz":-420,"elapsed":17,"user":{"displayName":"Quốc Hiệu Nguyễn","userId":"03425075822245856665"}}},"outputs":[],"source":["def tokenize(strings):\n","    #### YOUR CODE HERE ####\n","    # str -> List[str]\n","    return word_tokenize(strings, format=\"text\")\n","    #### END YOUR CODE #####\n","# print(tokenize(\"anh chả là Nhật ký SEA Games biết ngày 21/8: Ánh Viên thắng giòn giã bài cái ở vòng loại.\"))\n","# help(word_tokenize)"]},{"cell_type":"markdown","metadata":{"id":"-gQGmL4gxKd2"},"source":["### Question 2.4: Remove Stop Words\n","To remove stop words, we use a list of Vietnamese stop words stored in the file `./vietnamese-stopwords.txt`. Complete the following program:\n","- Check each word in the text (`strings`). If a word is not in the stop words list, add it to `doc_words`.\n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"aqStv2rPxKd3","executionInfo":{"status":"ok","timestamp":1769420127658,"user_tz":-420,"elapsed":97,"user":{"displayName":"Quốc Hiệu Nguyễn","userId":"03425075822245856665"}}},"outputs":[],"source":["from urllib.request import urlopen\n","url = \"https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt\"\n","raw_data = urlopen(url).read().decode(\"utf-8\")\n","STOPWORDS_SET = set(raw_data.split(\"\\n\"))\n","\n","def remove_stopwords(strings):\n","    #### YOUR CODE HERE ####\n","    words = [word for word in strings.split(\" \") if word not in STOPWORDS_SET]\n","    return \" \".join(words)\n","    #### END YOUR CODE #####\n","# print(remove_stopwords(tokenize(text_lowercase(clean_str(\"anh chả là Nhật ký SEA Games biết ngày 21/8: Ánh Viên thắng giòn giã bài cái ở vòng loại.\")))))"]},{"cell_type":"markdown","metadata":{"id":"jUNOKigIxKd4"},"source":["## Question 2.5: Build a Preprocessing Function\n","Hint: Call the functions `clean_str`, `text_lowercase`, `tokenize`, and `remove_stopwords` in order, then return the result from the function.\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"_vd-el91xKd_","executionInfo":{"status":"ok","timestamp":1769420127663,"user_tz":-420,"elapsed":4,"user":{"displayName":"Quốc Hiệu Nguyễn","userId":"03425075822245856665"}}},"outputs":[],"source":["def text_preprocessing(strings):\n","    #### YOUR CODE HERE ####\n","    cleaned_strings = clean_str(strings)\n","    lowercased_strings = text_lowercase(cleaned_strings)\n","    tokenized_strings = tokenize(lowercased_strings)\n","    sw_removed_strings = remove_stopwords(tokenized_strings)\n","    result = sw_removed_strings\n","\n","    return result\n","    #### END YOUR CODE #####"]},{"cell_type":"markdown","metadata":{"id":"1BGOqa1mxKeA"},"source":["## Question 3: Perform Preprocessing\n","Now, we will read the corpus from the file created in Question 1. After that, we will call the preprocessing function for each document in the corpus.\n","\n","Hint: Call the `text_preprocessing()` function with `doc_content` as the input parameter and save the result in the variable `temp1`.\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ii030C-RZtIv","outputId":"454f576c-e346-408f-9794-455be16064a9","executionInfo":{"status":"ok","timestamp":1769420236989,"user_tz":-420,"elapsed":109315,"user":{"displayName":"Quốc Hiệu Nguyễn","userId":"03425075822245856665"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","length of clean_docs =  3916\n","clean_docs[0]:\n","đôi giày thể_hiện tính_cách meghan_cleary , tác_giả sách tương_hợp hoàn_hảo , \" bất_cứ phụ_tùng trang_phục , đôi giày tiết_lộ trạng_thái tinh_thần phụ_nữ \" tìm_hiểu ý_nghĩa đôi giày ưng_ý kiểu giày lê_đế phẳng giỏi ngoại_giao , quan_tâm , chăm_sóc , thường_xuyên xoa_dịu , dàn hòa bất_đồng bạn_bè_bạn óc sáng_tạo nghiêm_túc kiểu giày cao_gót nhọn phối_hợp quyến_rũ truyền_thống hiện_đại , đầy_đủ sức_mạnh phụ_nữ tự_tin giày_hở gót năng_nổ , xông_xáo thực_sự , thường_xuyên thoăn_thoắt công_sở bữa tiệc hơi nghịch_ngợm một_chút , đánh_giá giày vải quyến_rũ điềm_đạm , người_yêu trò_chuyện thông_minh , quan_sát nhanh_nhẹn phát_triển bản_thân ngừng hoạt_động liên_tục hiểu_biết âm_nhạc , điện_ảnh giày sống tình có_lý , sợ đương_đầu vấn_đề gia_đình công_sở bạn_bè yêu quý_vẻ bình_dị , hài_hước\n"]}],"source":["#### YOUR CODE HERE ####\n","import gc\n","\n","clean_docs = []\n","with open(\"VNTC_khoahoc.txt\", \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        doc_content = line.strip()\n","        if not doc_content:\n","            continue\n","\n","        try:\n","            processed = text_preprocessing(doc_content)\n","            if processed:\n","                clean_docs.append(processed)\n","        except:\n","            continue\n","\n","        del doc_content\n","\n","gc.collect()\n","#### END YOUR CODE #####\n","\n","print(\"\\nlength of clean_docs = \", len(clean_docs))\n","print('clean_docs[0]:\\n' + clean_docs[0])"]},{"cell_type":"markdown","metadata":{"id":"SFhai6BwxKeB"},"source":["## Question 4: Save Preprocessed Data\n","Hint: Save the preprocessed data to a file named `dataset_name + '.clean.txt'`, where each document is written on a separate line.\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"xfHmSiRrxKeB","executionInfo":{"status":"ok","timestamp":1769420252895,"user_tz":-420,"elapsed":47,"user":{"displayName":"Quốc Hiệu Nguyễn","userId":"03425075822245856665"}}},"outputs":[],"source":["#### YOUR CODE HERE ####\n","filename = dataset_name + \".clean.txt\"\n","\n","with open(filename, \"w\", encoding=\"utf-8\") as f:\n","    for doc in clean_docs:\n","        f.write(doc + \"\\n\")\n","#### YOUR CODE HERE ####"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"}},"nbformat":4,"nbformat_minor":0}